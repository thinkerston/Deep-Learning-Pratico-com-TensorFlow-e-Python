{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 15 Geração de imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importação e tratamento da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalização\n",
    "\n",
    "train_images = (train_images -127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 60000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, 28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção do Gerador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100, ))) # 12544\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    \n",
    "    #7x7x128\n",
    "    model.add(layers.Convolution2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #14x14x64\n",
    "    model.add(layers.Convolution2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #28, 28, 1\n",
    "    model.add(layers.Convolution2DTranspose(1, (5,5), strides=(2,2), padding='same', \n",
    "                                            use_bias=False, activation='tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       "array([[-0.4356784 ,  1.0734032 , -0.31823984,  2.2940562 ,  0.6396277 ,\n",
       "        -0.7618794 ,  0.17057806,  0.36000797,  0.85921824,  0.52133906,\n",
       "        -0.49450153, -2.2327535 , -0.97321796,  2.302355  ,  0.4537355 ,\n",
       "        -1.0872172 ,  0.29157925,  0.32130066, -0.16641372,  0.38585863,\n",
       "        -1.428899  , -0.2688131 , -0.55741274, -2.4211612 , -0.13306059,\n",
       "        -0.40824297,  2.0249374 ,  0.7112749 ,  1.0415161 ,  1.4448376 ,\n",
       "        -0.54721975, -0.21250029,  1.6267625 , -1.6831043 ,  0.402821  ,\n",
       "         0.47421136,  0.4863388 ,  0.40591413,  0.40366778, -1.0243658 ,\n",
       "         0.07354546,  1.8920796 ,  0.38641974,  0.8044398 , -1.083878  ,\n",
       "         1.3214535 ,  0.4703477 , -2.949638  , -0.49541554,  0.9782772 ,\n",
       "         1.3640678 ,  0.7043544 , -1.2777969 ,  0.15232974, -0.83964264,\n",
       "        -0.91113836,  2.4476252 ,  0.7905993 , -1.5094221 ,  0.67734367,\n",
       "        -0.8412051 , -0.49926776, -1.6680443 , -1.3076415 , -0.19937502,\n",
       "         1.0498164 , -0.94547707, -0.9613553 , -2.1736894 , -0.7801187 ,\n",
       "         0.29134777,  1.198633  , -0.4217401 , -0.2552294 ,  0.6819944 ,\n",
       "        -1.216116  , -0.06502832,  0.39591205, -0.0163082 , -1.1376657 ,\n",
       "         0.40090796,  0.20855546,  0.8978653 , -0.6582633 , -0.37541747,\n",
       "         1.0360116 , -0.1464973 ,  1.1376479 ,  0.6573352 ,  1.4031384 ,\n",
       "        -0.36574787, -0.92379296,  0.00365599,  0.75639737, -0.8713277 ,\n",
       "        -1.3111145 ,  0.91303384, -1.9304253 ,  0.6529207 ,  1.5930512 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12544)             1254400   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_image = generator(noise, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 28, 28, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9dd6fea880>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(generate_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção do Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model ():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5,5), strides = (2,2), padding='same', input_shape=([28, 28, 1])))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5,5), strides = (2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00107432]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision = discriminator(generate_image, training=False)\n",
    "decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss (real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optmizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "discriminator_optmizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optmizer=generator_optmizer, \n",
    "                                 discriminator_optmizer=discriminator_optmizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "noise_dim = 100\n",
    "num_exemples_to_generate = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([num_exemples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_steps(images):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Chamando o gerador e passando os numeros aleatórios \n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        #passando as imagens fakes para o discriminador, para fazer a classificação\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        #Calculando a loss para o gerador e o discriminador\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "    #Calcular o gradiente das losses para atualizar os pesos\n",
    "    gradiente_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradiente_of_discriminator = disc_tape.gradient(disc_tape, discriminator.trainable_variables)\n",
    "    \n",
    "    #Aplicar otimizadores e ajustar os pesos\n",
    "    generator_optmizer.apply_gradients(zip(gradiente_of_generator, generator.trainable_variables))\n",
    "    discriminator_optmizer.apply_gradients(zip(gradiente_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for image_batch in dataset:\n",
    "            train_steps(image_batch)\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n",
    "        \n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        print('Time for epoch {} is {} sec. '.format((epoch+1), (time.time() - start)))\n",
    "        \n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epoch + 1, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
